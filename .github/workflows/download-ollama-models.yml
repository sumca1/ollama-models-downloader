name: Download Ollama Models (Offline Build)

on:
  workflow_dispatch:
    inputs:
      model:
        description: 'Model to download'
        required: true
        default: 'mistral:7b'
        type: choice
        options:
          - 'mistral:7b'
          - 'llama3.1:8b'
          - 'llama3.1:70b'
          - 'mixtral:8x7b'
          - 'llava:13b'
          - 'codellama:34b'
          - 'qwen2.5:72b'
          - 'deepseek-coder:33b'
      chunk_size_mb:
        description: 'Split size in MB (max 2000 for GitHub)'
        required: false
        default: '1900'
        type: string

jobs:
  download-models:
    runs-on: ubuntu-latest
    # Note: Free tier has 14GB disk. For larger models, consider:
    # runs-on: ubuntu-latest-4-cores (25GB) or ubuntu-latest-8-cores (75GB)
    # But these require GitHub Teams/Enterprise plan
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Check disk space
        run: |
          echo "ðŸ’¾ Available disk space:"
          df -h
          echo ""
          echo "ðŸ“Š /home/runner space:"
          df -h /home/runner | tail -1 | awk '{print "Free: " $4 " / Total: " $2}'

      - name: Install Ollama
        run: |
          curl -fsSL https://ollama.com/install.sh | sh
          
      - name: Start Ollama service
        run: |
          ollama serve &
          sleep 5
          
      - name: Download model
        run: |
          MODEL="${{ github.event.inputs.model }}"
          echo "ðŸ“¥ Downloading $MODEL..."
          ollama pull $MODEL
          echo "âœ… Download complete!"
          
      - name: Export and split model directly
        run: |
          mkdir -p ollama-models-export
          cd ollama-models-export
          
          CHUNK_SIZE_MB=${{ github.event.inputs.chunk_size_mb }}
          
          echo "ðŸ“¦ Creating tar and splitting in one go (saves space)..."
          # Create tar and pipe directly to split - no intermediate file!
          sudo tar -czf - -C /usr/share/ollama/.ollama models/ | split -b ${CHUNK_SIZE_MB}M - models.tar.gz.part_
          
          echo "ðŸ§¹ Cleaning up original models..."
          sudo rm -rf /usr/share/ollama/.ollama/models/*
          
          # Count chunks
          CHUNKS=$(ls -1 models.tar.gz.part_* 2>/dev/null | wc -l)
          
          if [ "$CHUNKS" -gt 1 ]; then
            echo "âœ‚ï¸ Created $CHUNKS chunks"
            TOTAL_SIZE_MB=$(du -sm . | cut -f1)
            
            # Create reassembly scripts
            echo '#!/bin/bash' > reassemble.sh
            echo 'echo "Reassembling archive..."' >> reassemble.sh
            echo 'cat models.tar.gz.part_* > models.tar.gz' >> reassemble.sh
            echo 'echo "Extracting..."' >> reassemble.sh
            echo 'tar -xzf models.tar.gz -C ~/.ollama/' >> reassemble.sh
            echo 'echo "Done!"' >> reassemble.sh
            chmod +x reassemble.sh
            
            echo 'Write-Host "Reassembling..." -ForegroundColor Cyan' > reassemble.ps1
            echo 'Get-ChildItem models.tar.gz.part_* | Sort-Object | Get-Content -Raw -AsByteStream | Set-Content models.tar.gz -AsByteStream' >> reassemble.ps1
            echo 'Write-Host "Extracting..." -ForegroundColor Cyan' >> reassemble.ps1
            echo 'tar -xzf models.tar.gz -C $env:USERPROFILE\.ollama\' >> reassemble.ps1
            echo 'Write-Host "Done!" -ForegroundColor Green' >> reassemble.ps1
          else
            echo "ðŸ“¦ Single file (no splitting needed)"
            mv models.tar.gz.part_aa models.tar.gz
            TOTAL_SIZE_MB=$(du -sm models.tar.gz | cut -f1)
          fi
          
          # Create metadata
          cat > METADATA.json << EOF
          {
            "downloaded_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
            "model": "${{ github.event.inputs.model }}",
            "ollama_version": "$(ollama --version | head -n1)",
            "total_size_mb": ${TOTAL_SIZE_MB},
            "chunks": ${CHUNKS}
          }
          EOF
          
          echo "ðŸ“Š Total size: ${TOTAL_SIZE_MB}MB in $CHUNKS part(s)"
          cd ..
          

          
      - name: Create README
        run: |
          cat > ollama-models-export/README.md << 'EOF'
          # ðŸ¤– Ollama Model: ${{ github.event.inputs.model }}
          
          ## ðŸ“¦ Package Contents
          
          - **Model**: `${{ github.event.inputs.model }}`
          - **Downloaded**: $(date -u +%Y-%m-%d)
          - **Split chunks**: See METADATA.json
          
          ## ðŸ“¥ Installation Instructions
          
          ### If archive is split (multiple `.part_*` files):
          
          #### Windows PowerShell:
          ```powershell
          # Run the reassembly script
          .\reassemble.ps1
          ```
          
          #### Linux/Mac:
          ```bash
          # Run the reassembly script
          chmod +x reassemble.sh
          ./reassemble.sh
          ```
          
          ### If archive is NOT split (single `models.tar.gz`):
          
          #### Windows:
          ```powershell
          tar -xzf models.tar.gz -C $env:USERPROFILE\.ollama\
          ```
          
          #### Linux/Mac:
          ```bash
          tar -xzf models.tar.gz -C ~/.ollama/
          ```
          
          ## âœ… Verify Installation
          
          ```bash
          ollama list
          ollama run ${{ github.event.inputs.model }}
          ```
          
          ## ðŸ“Š Model Information
          
          | Model | Size | Best For |
          |-------|------|----------|
          | llama3.1:70b | 40GB | Hebrew, IVR, Complex tasks |
          | mixtral:8x7b | 26GB | Fast + Smart, Multi-domain |
          | llava:13b | 10GB | Vision AI, OCR, Images |
          | codellama:34b | 20GB | Advanced Programming |
          | qwen2.5:72b | 42GB | Multilingual (Hebrew+Chinese) |
          | deepseek-coder:33b | 19GB | GPT-4 level coding |
          | llama3.1:8b | 4.9GB | â­ Balanced, Recommended start |
          | mistral:7b | 4.1GB | Lightweight, Fast tests |
          
          ## ðŸ”§ Using Python Installer
          
          ```bash
          python install_ollama_models.py ./ollama-models-export
          ```
          
          EOF
          
      - name: Upload model as artifact
        uses: actions/upload-artifact@v4
        with:
          name: ollama-model-${{ github.event.inputs.model }}-${{ github.run_number }}
          path: ollama-models-export/
          retention-days: 90
          compression-level: 0
          
      - name: Display download summary
        run: |
          echo "========================================="
          echo "âœ… Model Downloaded Successfully!"
          echo "========================================="
          echo "ðŸ“¦ Model: ${{ github.event.inputs.model }}"
          echo "ðŸ“Š Total size: $(du -sh ollama-models-export | cut -f1)"
          
          if [ -f ollama-models-export/models.tar.gz.part_aa ]; then
            echo "âœ‚ï¸ Archive was split into chunks"
            echo "ðŸ“ Use reassemble script before extraction"
          else
            echo "ðŸ“¦ Archive is single file (no splitting needed)"
          fi
          
          echo ""
          echo "ðŸ”½ Download from:"
          echo "   GitHub â†’ Actions â†’ This workflow â†’ Artifacts"
          echo ""
          echo "ðŸ“¥ Artifact name:"
          echo "   ollama-model-${{ github.event.inputs.model }}-${{ github.run_number }}"
